# -*- coding: utf-8 -*-
"""detectron2_trainingWithFreezing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P_d9bUmrKexUu4haEFvXgBCvLITRjmbP
"""

import torch, torchvision
print(torch.__version__, torch.cuda.is_available())
#import torch
#assert torch.__version__.startswith("1.7")   # need to manually install torch 1.7 if Colab changes its default version
import detectron2


# import some common libraries
import numpy as np
import os, json, random

# import some common detectron2 utilities
#from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
#from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
#from detectron2.utils.visualizer import Visualizer
from detectron2.engine import DefaultTrainer
import json

from detectron2.data.datasets import register_coco_instances

dataroot='./sanskrit-layout-gt-manual-old'
traindata=dataroot+'/train'
testdata=dataroot+'/test'
valdata=dataroot+'/val'
trainjson=traindata+'/train.json'
testjson=testdata+'/test.json'
valjson=valdata+'/val.json'
trainimages=traindata+'/images'
testimages=testdata+'/images'
valimages=valdata+'/images'

register_coco_instances("train_data", {}, trainjson, trainimages)
register_coco_instances("test_data", {}, testjson, testimages)
register_coco_instances("val_data", {}, valjson, valimages)



from detectron2.engine.hooks import HookBase
from detectron2.evaluation import inference_context
from detectron2.utils.logger import log_every_n_seconds
from detectron2.data import DatasetMapper, build_detection_test_loader
import detectron2.utils.comm as comm
import torch
import time
import datetime
import logging
from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg
from detectron2.evaluation import COCOEvaluator

from collections import OrderedDict
import torch
from torch.nn.parallel import DistributedDataParallel
import detectron2.utils.comm as comm
from detectron2.checkpoint import DetectionCheckpointer, PeriodicCheckpointer
from detectron2.config import get_cfg
from detectron2.data import (
    MetadataCatalog,
    build_detection_test_loader,
    build_detection_train_loader,
)
from detectron2.engine import default_argument_parser, default_setup, launch
from detectron2.evaluation import (
    COCOEvaluator,
    inference_on_dataset,
    print_csv_format,
)
from detectron2.modeling import build_model
from detectron2.solver import build_lr_scheduler, build_optimizer
from detectron2.utils.events import (
    CommonMetricPrinter,
    EventStorage,
    JSONWriter,
    TensorboardXWriter,
)

from tqdm import tqdm
from detectron2 import model_zoo
from detectron2.structures import BoxMode
import detectron2.utils.comm as comm
from detectron2.engine import default_argument_parser, default_setup, default_writers, launch

#config_file = "/content/config/DLA_mask_rcnn_X_101_32x8d_FPN_3x.yaml"
  #model_file = "/content/models/model_final_trimmed.pth"
cfg = get_cfg()
#cfg._open_cfg(config_file)
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("train_data",)
cfg.DATASETS.TEST = ('val_data',)
cfg.INPUT.MAX_SIZE_TEST = 1247
cfg.INPUT.MAX_SIZE_TRAIN = 1247
cfg.INPUT.MIN_SIZE_TEST = 743
cfg.INPUT.MIN_SIZE_TRAIN = (743,)
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")  # Let training initialize from model zoo
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.EARLY_STOPPING_MONITOR = 'val_loss'
cfg.EARLY_STOPPING_PATIENCE = 10
cfg.SOLVER.BASE_LR = 0.001  # pick a good LR
cfg.SOLVER.STEPS = []        # do not decay learning rate
cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[1.0,3.0,10.0]]
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 2000   # faster, and good enough for this toy dataset (default: 512)
cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.7
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5  
print(cfg)
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)

#trainer = SanskritTrainer(cfg)  
#trainer.resume_or_load(resume=True)
#trainer.train()

logger = logging.getLogger("detectron2")

def do_evaluate(cfg, model):
    results = OrderedDict()
    for dataset_name in cfg.DATASETS.TEST:
        print(dataset_name)
        data_loader = build_detection_test_loader(cfg, dataset_name)
        evaluator = COCOEvaluator(dataset_name, cfg, True, "inference")
        results_i = inference_on_dataset(model, data_loader, evaluator)
        results[dataset_name] = results_i
        print(results)
        if comm.is_main_process():
            logger.info("Evaluation results for {} in csv format:".format(dataset_name))
            print_csv_format(results_i)
    if len(results) == 1:
        results = list(results.values())[0]
    return results

def do_loss(cfg, model):
  losses = []
  data_loader = build_detection_test_loader(cfg, cfg.DATASETS.TEST[0],DatasetMapper(cfg,True))
  for idx, inputs in enumerate(data_loader):
    metrics_dict = model(inputs)
    metrics_dict = {
        k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)
        for k, v in metrics_dict.items()
    }
    total_losses_reduced = sum(loss for loss in metrics_dict.values())
    losses.append(total_losses_reduced)
  mean_loss = np.mean(losses)
  return mean_loss

def train_model(freeze_at,iterations,rpn_threshold, val_loss_check):
  cfg.MODEL.BACKBONE.FREEZE_AT = freeze_at
  cfg.SOLVER.MAX_ITER = iterations
  cfg.MODEL.RPN.NMS_THRESH = rpn_threshold
  cfg.TEST.EVAL_PERIOD = 500

  check_loss = val_loss_check[0]
  loss_floor, loss_ceiling = val_loss_check[1], val_loss_check[2]

  resume = True 
  model = build_model(cfg)
  model.train()
  optimizer = build_optimizer(cfg, model)
  scheduler = build_lr_scheduler(cfg, optimizer)
  max_iter = cfg.SOLVER.MAX_ITER
  # checkpointer helps in saving --> checkpointer.save('filename.pth')
  checkpointer = DetectionCheckpointer(
          model, cfg.OUTPUT_DIR, optimizer=optimizer, scheduler=scheduler
      )
  start_iter = (
          checkpointer.resume_or_load(cfg.MODEL.WEIGHTS, resume=resume).get("iteration", -1) + 1
      )
  periodic_checkpointer = PeriodicCheckpointer(
          checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD, max_iter=max_iter
      )
  # writer object saves loss metrics
  writers = default_writers(cfg.OUTPUT_DIR, max_iter) if comm.is_main_process() else []
  #writer = JSONWriter(os.path.join( "test_metrics2.json"))
  train_data_loader = build_detection_train_loader(cfg)

  loss_list = []
  eval_list = []
  total_loss_list = []
  val_loss_list = []
  #os.makedirs(OUTPUT_DIR,exist_ok=True)
  logger = logging.getLogger("detectron2")

  with EventStorage(start_iter) as storage:
      for data, iteration in zip(train_data_loader, range(start_iter, max_iter)):
          storage.iter = iteration
          storage.step() #--.needed
  #       storing training loss-> 
          loss_dict = model(data)
          losses = sum(loss for loss in loss_dict.values())
          assert torch.isfinite(losses).all(), loss_dict

          loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}
          losses_reduced = sum(loss for loss in loss_dict_reduced.values())
          if comm.is_main_process():
            storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)

          loss_list.append(loss_dict_reduced)
          total_loss_list.append(losses_reduced)
  #       training 
          optimizer.zero_grad()
          losses.backward()
          optimizer.step()
          storage.put_scalar("lr", optimizer.param_groups[0]["lr"], smoothing_hint=False)
          scheduler.step()

          #writer.write()
          if (cfg.TEST.EVAL_PERIOD > 0 and (iteration + 1) % cfg.TEST.EVAL_PERIOD == 0 and iteration != max_iter - 1):
              results = do_evaluate(cfg, model)
              eval_list.append(results["bbox"])
              
              val_loss = do_loss(cfg, model)
              val_loss_list.append(val_loss)

          if iteration - start_iter > 5 and (
                  (iteration + 1) % 20 == 0 or iteration == max_iter - 1
              ):
                for writer in writers:
                  writer.write()
                  
          if len(val_loss_list)>=check_loss:
              stop_flag = all(l >= loss_floor and l < loss_ceiling for l in val_loss_list[-int(check_loss):]) 
              if stop_flag: 
                print("Stop Condition for training has been met.")
                checkpointer.save("model_final_freezelayer"+str(freeze_at))
                break 

          periodic_checkpointer.step(iteration)

#test with few iterations - 
#val_loss_check = [2,1.0,5.0] 
#val_loss_check defines the validation loss conditions that can stop training. it is a list with 3 values - 
#first: checks loss computed for n iterations for the stop condition
#second and third value are the loss lower and upper limits 
#train_model(5, 100, 0.5, val_loss_check)

layers_count = [5,3,0]
rpn_threshold = [0.5,0.7,0.9]
iterations_number = [15000,15000,15000]
i = 0
iterations = 0
#val_loss_check defines the validation loss conditions that can stop training. it is a list with 3 values - 
#first: checks loss computed for n iterations for the stop condition
#second and third value are the loss lower and upper limits 
val_loss_check = [5, 0, 0.40]
for freeze_at in layers_count:
  iterations=iterations_number[i]
  if(freeze_at == 0):
      cfg.SOLVER.BASE_LR = 0.0003  # pick a good LR
  train_model(freeze_at,iterations,rpn_threshold[i], val_loss_check)
  i+=1
